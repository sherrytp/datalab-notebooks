{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here\n",
    "\n",
    "### This notebook contains basic examples to demonstrate use of Spark by Python. \n",
    "\n",
    "### See the [Spark Python API Docs](http://spark.apache.org/docs/latest/api/python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download Docker https://www.docker.com\n",
    "- Install Docker\n",
    "- Start the _Docker Quickstart Terminal_\n",
    "- Run the command \"docker run hello-world\"\n",
    "- Run \"docker run -d -p 8888:8888 -v [yourdir]:/home/jovyan/work jupyter/pyspark-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The `pyspark` module is the main entry point and contains routines for connecting to a cluster. \n",
    "\n",
    "### Run the `SparkContext` function to obtain a connection, called a _Spark context_, to the Spark cluster.\n",
    "\n",
    "### In this case we run the Spark process locally  and so pass the string 'local[*]' to the `SparkContext` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context: <pyspark.context.SparkContext object at 0x7f9a9469eba8>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "print('Spark context:',sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Datasets (RDD)\n",
    "\n",
    "An RDD can be thought of as a _Spark_ dataset. For example, consider the familiar `iris` dataset from __R__.\n",
    "\n",
    "1. The dataset is broken into pieces. \n",
    "1. Multiple copies of each piece are distributed to multiple computers. \n",
    "\n",
    "This means that: \n",
    "\n",
    "1. The pieces of the dataset can be analyzed by multiple computers (simultaneously)\n",
    "1. If one computer goes down then the _entire_ dataset is still available. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three basic types of operations on data in Spark: \n",
    "\n",
    "1. Load data into an RDD on the spark cluster\n",
    "1. Transform data from one RDD (on the Spark cluster) into another RDD. \n",
    "1. Pull data from an RDD into the _driver_ (often your laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code loads the data `[1, 2, 3]` into a Spark RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code pulls the data from an RDD back to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The functions that _transform data_ are called \"transformations\". We will see some of them in a moment.\n",
    "\n",
    "### The functions that pull data into the driver are called \"actions\". The `collect` function is an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can keep track of RDDs with variables, and then run functions on those RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_rdd = sc.parallelize([1,2,3])\n",
    "an_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example, loads data from the file `Dance.txt` with the `textFile` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_line = sc.textFile(\"Dance_5lines.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data from `Dance.txt` isn't really sent to the spark cluster by the `textFile` command or by other _load_ commands (when they are called). Spark is _lazy_ in that it doesn't do any work until you call an action function. \n",
    "\n",
    "### Function `take` is an action function the sends to you (the driver) the first 5 (in this case) elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'The Best Dance of 2015']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `collect` is an action that sends  __all__ elements of the RDD to the driver. In general, this isn't a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The Best Dance of 2015',\n",
       " 'By ALASTAIR MACAULAY, GIA KOURLAS, BRIAN SEIBERT and SIOBHAN BURKEDEC. 9, 2015',\n",
       " 'Photo',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `count` is an action that returns the number of elements in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `takeSample` is another action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'By ALASTAIR MACAULAY, GIA KOURLAS, BRIAN SEIBERT and SIOBHAN BURKEDEC. 9, 2015']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.takeSample(withReplacement=False, num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See [Actions in the Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html#actions) for a list of common actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll look at _transformations_. The first example is a very simple __map-reduce__ statment.\n",
    "\n",
    "### The `map` function is a transformation and applies a function to each element of the input RDD in order produce the output RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 22, 78, 5, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.map(lambda line:  len(line)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `reduce` function is an action and replaces two elements of the input RDD with the result obtained by applying the anonymous function (argment to `reduce`) to those two elements, which produces a single element. The function `reduce` function returns where there is only element remaining and returns that element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.map(lambda line:  len(line)).reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `filter` is a transformation which returns an RDD with only those elements from the input RDD for which the anonymous function returns `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Best Dance of 2015',\n",
       " 'By ALASTAIR MACAULAY, GIA KOURLAS, BRIAN SEIBERT and SIOBHAN BURKEDEC. 9, 2015']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.filter(lambda line: \"2015\" in line).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the `split` function on strings. It takes a string as input and returns a list of strings (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['February', '26,', '2016']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"February 26,         2016\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can apply this transformation to each element of the RDD of lines from `Dance_5lines.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['The', 'Best', 'Dance', 'of', '2015'],\n",
       " ['By',\n",
       "  'ALASTAIR',\n",
       "  'MACAULAY,',\n",
       "  'GIA',\n",
       "  'KOURLAS,',\n",
       "  'BRIAN',\n",
       "  'SEIBERT',\n",
       "  'and',\n",
       "  'SIOBHAN',\n",
       "  'BURKEDEC.',\n",
       "  '9,',\n",
       "  '2015']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.map(lambda line: line.split()).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is nice, but not what we want, which is an RDD whose elements are single words from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Best', 'Dance', 'of', '2015', 'By', 'ALASTAIR']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_line.flatMap(lambda line: line.split()).take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store this new RDD of words in `rdd_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_word = rdd_line.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Best', 'Dance', 'of', '2015', 'By', 'ALASTAIR']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next example, counts occurences of words in the file. \n",
    "\n",
    "### The first step is to create an RDD whose elements are tuples (a Python term) which consist of a word (the key) and the number `1` (the corresponding value). This RDD is stored in `rdd_word_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1), ('Best', 1), ('Dance', 1), ('of', 1), ('2015', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word_1 = rdd_word.map(lambda word: (word,1))\n",
    "rdd_word_1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this RDD has containing word counts, but with duplicates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `reduceByKey` is a tranformation which applies its argument function to any two tuples with the same key (word) and returns a tuple with that key and with the sum of the values (word counts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_word_count = rdd_word_1.reduceByKey(lambda count_1, count_2: count_1 + count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Best', 1),\n",
       " ('ALASTAIR', 1),\n",
       " ('Dance', 1),\n",
       " ('GIA', 1),\n",
       " ('BURKEDEC.', 1),\n",
       " ('Photo', 1),\n",
       " ('9,', 1),\n",
       " ('and', 1),\n",
       " ('BRIAN', 1),\n",
       " ('By', 1),\n",
       " ('KOURLAS,', 1),\n",
       " ('of', 1),\n",
       " ('2015', 2),\n",
       " ('MACAULAY,', 1),\n",
       " ('SEIBERT', 1),\n",
       " ('The', 1),\n",
       " ('SIOBHAN', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2015', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_word_count.filter(lambda key_val: key_val[1]>1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this next example, we inspect the `iris` dataset. To do so we will create a dataframe and will use SQL like functions. \n",
    "\n",
    "### First load functions `SQLContext` and `Row` from the `pyspark.sql` module and create an SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f9a7704d470>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the `iris_noheader.csv` file as a text file. \n",
    "\n",
    "### The `iris_text` RDD contains one element for each line of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.1,3.5,1.4,0.2,\"setosa\"',\n",
       " '4.9,3,1.4,0.2,\"setosa\"',\n",
       " '4.7,3.2,1.3,0.2,\"setosa\"']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_text = sc.textFile(\"iris_noheader.csv\")\n",
    "iris_text.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now split each line by commas to create an RDD of lists of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', '\"setosa\"'],\n",
       " ['4.9', '3', '1.4', '0.2', '\"setosa\"'],\n",
       " ['4.7', '3.2', '1.3', '0.2', '\"setosa\"']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_line = iris_text.map(lambda l: l.split(\",\"))\n",
    "iris_line.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create an RDD of tuples with decimal and string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PetalLength=1.4, PetalWidth=0.2, SepalLength=5.1, SepalWidth=3.5, Species='\"setosa\"'),\n",
       " Row(PetalLength=1.4, PetalWidth=0.2, SepalLength=4.9, SepalWidth=3.0, Species='\"setosa\"'),\n",
       " Row(PetalLength=1.3, PetalWidth=0.2, SepalLength=4.7, SepalWidth=3.2, Species='\"setosa\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_row  = iris_line.map(lambda p: Row(SepalLength=float(p[0]),\n",
    "                                        SepalWidth=float(p[1]),\n",
    "                                        PetalLength=float(p[2]),\n",
    "                                        PetalWidth=float(p[3]),\n",
    "                                        Species=p[4]))\n",
    "iris_row.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create a dataframe from this RDD of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PetalLength: double (nullable = true)\n",
      " |-- PetalWidth: double (nullable = true)\n",
      " |-- SepalLength: double (nullable = true)\n",
      " |-- SepalWidth: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(PetalLength=1.4, PetalWidth=0.2, SepalLength=5.1, SepalWidth=3.5, Species='\"setosa\"'),\n",
       " Row(PetalLength=1.4, PetalWidth=0.2, SepalLength=4.9, SepalWidth=3.0, Species='\"setosa\"'),\n",
       " Row(PetalLength=1.3, PetalWidth=0.2, SepalLength=4.7, SepalWidth=3.2, Species='\"setosa\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = sqlContext.createDataFrame(iris_row)\n",
    "iris_df.printSchema()\n",
    "iris_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can use the functions that operate on dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|PetalLength|PetalWidth|     Species|\n",
      "+-----------+----------+------------+\n",
      "|        1.7|       0.4|    \"setosa\"|\n",
      "|        1.6|       0.2|    \"setosa\"|\n",
      "|        1.7|       0.3|    \"setosa\"|\n",
      "|        1.7|       0.2|    \"setosa\"|\n",
      "|        1.7|       0.5|    \"setosa\"|\n",
      "|        1.9|       0.2|    \"setosa\"|\n",
      "|        1.6|       0.2|    \"setosa\"|\n",
      "|        1.6|       0.4|    \"setosa\"|\n",
      "|        1.6|       0.2|    \"setosa\"|\n",
      "|        1.6|       0.2|    \"setosa\"|\n",
      "|        1.6|       0.6|    \"setosa\"|\n",
      "|        1.9|       0.4|    \"setosa\"|\n",
      "|        1.6|       0.2|    \"setosa\"|\n",
      "|        4.7|       1.4|\"versicolor\"|\n",
      "|        4.5|       1.5|\"versicolor\"|\n",
      "|        4.9|       1.5|\"versicolor\"|\n",
      "|        4.0|       1.3|\"versicolor\"|\n",
      "|        4.6|       1.5|\"versicolor\"|\n",
      "|        4.5|       1.3|\"versicolor\"|\n",
      "|        4.7|       1.6|\"versicolor\"|\n",
      "+-----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.select(\"PetalLength\",\"PetalWidth\",\"Species\").filter(iris_df['PetalLength'] > 1.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|       PetalLength|        PetalWidth|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               150|               150|\n",
      "|   mean|3.7580000000000027| 1.199333333333334|\n",
      "| stddev|1.7652982332594662|0.7622376689603467|\n",
      "|    min|               1.0|               0.1|\n",
      "|    max|               6.9|               2.5|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.describe(\"PetalLength\",\"PetalWidth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|     Species|count|\n",
      "+------------+-----+\n",
      "|    \"setosa\"|   50|\n",
      "|\"versicolor\"|   50|\n",
      "| \"virginica\"|   50|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.groupBy(\"Species\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See [pyspark.sql documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Species='\"setosa\"', avg(PetalLength)=1.4620000000000002),\n",
       " Row(Species='\"versicolor\"', avg(PetalLength)=4.26),\n",
       " Row(Species='\"virginica\"', avg(PetalLength)=5.552)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as pf\n",
    "iris_df.select(\"PetalLength\",\"Species\"). \\\n",
    "groupBy(\"Species\"). \\\n",
    "agg(pf.mean(iris_df.PetalLength)). \\\n",
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_df.registerTempTable(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PetalLength: double, PetalWidth: double, SepalLength: double, SepalWidth: double, Species: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * from iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+--------+\n",
      "|PetalLength|PetalWidth|SepalLength|SepalWidth| Species|\n",
      "+-----------+----------+-----------+----------+--------+\n",
      "|        1.4|       0.2|        5.1|       3.5|\"setosa\"|\n",
      "|        1.4|       0.2|        4.9|       3.0|\"setosa\"|\n",
      "|        1.3|       0.2|        4.7|       3.2|\"setosa\"|\n",
      "|        1.5|       0.2|        4.6|       3.1|\"setosa\"|\n",
      "|        1.4|       0.2|        5.0|       3.6|\"setosa\"|\n",
      "|        1.7|       0.4|        5.4|       3.9|\"setosa\"|\n",
      "|        1.4|       0.3|        4.6|       3.4|\"setosa\"|\n",
      "|        1.5|       0.2|        5.0|       3.4|\"setosa\"|\n",
      "|        1.4|       0.2|        4.4|       2.9|\"setosa\"|\n",
      "|        1.5|       0.1|        4.9|       3.1|\"setosa\"|\n",
      "|        1.5|       0.2|        5.4|       3.7|\"setosa\"|\n",
      "|        1.6|       0.2|        4.8|       3.4|\"setosa\"|\n",
      "|        1.4|       0.1|        4.8|       3.0|\"setosa\"|\n",
      "|        1.1|       0.1|        4.3|       3.0|\"setosa\"|\n",
      "|        1.2|       0.2|        5.8|       4.0|\"setosa\"|\n",
      "|        1.5|       0.4|        5.7|       4.4|\"setosa\"|\n",
      "|        1.3|       0.4|        5.4|       3.9|\"setosa\"|\n",
      "|        1.4|       0.3|        5.1|       3.5|\"setosa\"|\n",
      "|        1.7|       0.3|        5.7|       3.8|\"setosa\"|\n",
      "|        1.5|       0.3|        5.1|       3.8|\"setosa\"|\n",
      "+-----------+----------+-----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * from iris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+--------+\n",
      "|PetalLength|PetalWidth|SepalLength|SepalWidth| Species|\n",
      "+-----------+----------+-----------+----------+--------+\n",
      "|        1.5|       0.4|        5.7|       4.4|\"setosa\"|\n",
      "|        1.5|       0.1|        5.2|       4.1|\"setosa\"|\n",
      "|        1.4|       0.2|        5.5|       4.2|\"setosa\"|\n",
      "+-----------+----------+-----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * from iris where SepalWidth > 4.0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
